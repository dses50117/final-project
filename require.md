1. CRISP-DM å°ˆæ¡ˆåˆ†æ (é‡å° NTHU + DDD è³‡æ–™é›†)
1. Business Understanding (å•†æ¥­ç†è§£)

ç›®æ¨™ï¼šæ•´åˆå¤šä¾†æºè³‡æ–™ï¼Œå»ºç«‹ä¸€å€‹é«˜é­¯æ£’æ€§çš„ç–²å‹åµæ¸¬æ¨¡å‹ï¼Œä¸¦åœ¨ Streamlit ä¸Šå¯¦ç¾ã€Œå³æ™‚ Webcamã€èˆ‡ã€Œå½±ç‰‡æª”ã€é›™é‡ç›£æ§ã€‚


ç­–ç•¥ï¼šåˆ©ç”¨ PPT æåŠçš„ "MediaPipe + XGBoost" è¼•é‡åŒ–ç­–ç•¥ï¼Œç¢ºä¿åœ¨æ²’æœ‰ GPU çš„ç­†é›»ä¸Šä¹Ÿèƒ½è·‘å‡º 30 FPS ã€‚


2. Data Understanding (è³‡æ–™ç†è§£)

è³‡æ–™ä¾†æº A (NTHU-DDD2)ï¼šæ¨¡æ“¬é§•é§›å®¤ç’°å¢ƒï¼ŒåŒ…å«å¤œé–“ã€æˆ´çœ¼é¡ç­‰å›°é›£æ¨£æœ¬ (ç´„ 6.6 è¬å¼µ)ã€‚

è³‡æ–™ä¾†æº B (DDD)ï¼šèƒŒæ™¯è¼ƒå–®ç´”ï¼Œç‰¹å¾µæ¸…æ™° (ç´„ 4.1 è¬å¼µ)ã€‚

æ•´åˆæŒ‘æˆ°ï¼šå…©è€…è§£æåº¦ä¸åŒï¼Œä½†å› ç‚ºæˆ‘å€‘æ˜¯ç”¨ MediaPipe æŠ“ã€Œç›¸å°åº§æ¨™ (Landmarks)ã€ï¼Œæ‰€ä»¥è§£æåº¦å·®ç•°ä¸å½±éŸ¿ï¼Œå¯ä»¥ç›´æ¥åˆä½µè¨“ç·´ã€‚

3. Data Preparation (è³‡æ–™æº–å‚™)

ç‰¹å¾µå·¥ç¨‹ï¼šä¸ä½¿ç”¨åŸå§‹åƒç´  (Pixels)ï¼Œè€Œæ˜¯æå– EAR (çœ¼ç›ç¸±æ©«æ¯”) èˆ‡ MAR (å˜´å·´ç¸±æ©«æ¯”)ã€‚

æ¸…æ´—ï¼šéœ€å‰”é™¤ MediaPipe æŠ“ä¸åˆ°è‡‰çš„åœ–ç‰‡ (Outliers)ã€‚

4. Modeling (æ¨¡å‹å»ºç«‹)

æ¼”ç®—æ³•ï¼šXGBoost Classifierã€‚å®ƒèƒ½å¾ˆå¥½åœ°è™•ç†é€™ 10 è¬ç­†çµæ§‹åŒ–æ•¸æ“š (Table data)ï¼Œè¨“ç·´é€Ÿåº¦å¿«ä¸”æº–ç¢ºã€‚

5. Evaluation (è©•ä¼°)

ä½¿ç”¨æ··æ·†çŸ©é™£ (Confusion Matrix) ç¢ºèªæ¨¡å‹æ˜¯å¦èƒ½æ­£ç¢ºå€åˆ† NTHU çš„å›°é›£æ¨£æœ¬ã€‚

6. Deployment (éƒ¨ç½²)

Streamlit Appï¼šè¨­è¨ˆã€Œå³æ™‚ç›£æ§æ¨¡å¼ã€èˆ‡ã€Œå½±ç‰‡åˆ†ææ¨¡å¼ã€ã€‚

2. VS Code å°ˆæ¡ˆå¯¦ä½œ (å®Œæ•´ç¨‹å¼ç¢¼)
è«‹åœ¨ VS Code å»ºç«‹ä¸€å€‹è³‡æ–™å¤¾ï¼Œä¸¦ç¢ºèªå·²å®‰è£å¥—ä»¶ï¼š pip install opencv-python mediapipe pandas scikit-learn xgboost streamlit joblib tqdm

æ­¥é©Ÿä¸€ï¼šè³‡æ–™æ•´ç†èˆ‡ç‰¹å¾µæå– (1_process_data.py)
é€™å€‹è…³æœ¬æœƒè‡ªå‹•è®€å–å…©å€‹è³‡æ–™é›†ï¼Œæå–ç‰¹å¾µä¸¦åˆä½µæˆä¸€å€‹ CSVã€‚

âš ï¸ è«‹æ³¨æ„è³‡æ–™å¤¾çµæ§‹è¨­å®šï¼š å‡è¨­æ‚¨çš„ç›®éŒ„çµæ§‹å¦‚ä¸‹ (è«‹ä¾ç…§æ‚¨çš„å¯¦éš›è·¯å¾‘ä¿®æ”¹ DATASETS è®Šæ•¸)ï¼š

Plaintext

Project/
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ nthuddd2/
â”‚   â”‚   â”œâ”€â”€ Drowsy/ (æ”¾åœ–ç‰‡)
â”‚   â”‚   â””â”€â”€ Non Drowsy/
â”‚   â”œâ”€â”€ ddd/
â”‚   â”‚   â”œâ”€â”€ Drowsy/
â”‚   â”‚   â””â”€â”€ Non Drowsy/
Python

# 1_process_data.py
import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import os
from scipy.spatial import distance as dist
from tqdm import tqdm  # é€²åº¦æ¢

# --- è¨­å®šè·¯å¾‘ (è«‹ä¾æ‚¨çš„å¯¦éš›ä¸‹è¼‰ä½ç½®ä¿®æ”¹) ---
DATASETS = {
    "nthu": {
        "drowsy": "raw_data/nthuddd2/Drowsy",
        "alert": "raw_data/nthuddd2/Non Drowsy"
    },
    "ddd": {
        "drowsy": "raw_data/ddd/Drowsy",
        "alert": "raw_data/ddd/Non Drowsy"
    }
}

# --- MediaPipe è¨­å®š ---
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=True,  # è™•ç†éœæ…‹åœ–ç‰‡æ¨¡å¼
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5
)

# --- é—œéµé»ç´¢å¼• ---
LEFT_EYE = [362, 385, 387, 263, 373, 380]
RIGHT_EYE = [33, 160, 158, 133, 153, 144]
MOUTH = [78, 81, 13, 311, 308, 402, 14, 178]

def calculate_ear(eye_points, landmarks):
    p = [landmarks[i] for i in eye_points]
    A = dist.euclidean((p[1].x, p[1].y), (p[5].x, p[5].y))
    B = dist.euclidean((p[2].x, p[2].y), (p[4].x, p[4].y))
    C = dist.euclidean((p[0].x, p[0].y), (p[3].x, p[3].y))
    return (A + B) / (2.0 * C) if C != 0 else 0

def calculate_mar(mouth_points, landmarks):
    p = [landmarks[i] for i in mouth_points]
    A = dist.euclidean((p[1].x, p[1].y), (p[7].x, p[7].y))
    B = dist.euclidean((p[2].x, p[2].y), (p[6].x, p[6].y))
    C = dist.euclidean((p[3].x, p[3].y), (p[5].x, p[5].y))
    D = dist.euclidean((p[0].x, p[0].y), (p[4].x, p[4].y))
    return (A + B + C) / (2.0 * D) if D != 0 else 0

data_list = []

print("ğŸš€ é–‹å§‹è™•ç†å…©å€‹è³‡æ–™é›†ï¼Œé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")

# éæ­·æ‰€æœ‰è³‡æ–™é›†èˆ‡é¡åˆ¥
for dataset_name, paths in DATASETS.items():
    for label_name, folder_path in paths.items():
        label = 1 if label_name == "drowsy" else 0
        
        if not os.path.exists(folder_path):
            print(f"âš ï¸ è·³éï¼šæ‰¾ä¸åˆ°è·¯å¾‘ {folder_path}")
            continue

        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        print(f"æ­£åœ¨è™•ç† {dataset_name} - {label_name} (å…± {len(files)} å¼µ)...")

        for filename in tqdm(files):
            filepath = os.path.join(folder_path, filename)
            image = cv2.imread(filepath)
            if image is None: continue

            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

            if results.multi_face_landmarks:
                landmarks = results.multi_face_landmarks[0].landmark
                
                ear_left = calculate_ear(LEFT_EYE, landmarks)
                ear_right = calculate_ear(RIGHT_EYE, landmarks)
                mar = calculate_mar(MOUTH, landmarks)
                
                # åŠ å…¥è³‡æ–™åˆ—è¡¨
                data_list.append([ear_left, ear_right, mar, label])

# è½‰ç‚º DataFrame ä¸¦å„²å­˜
df = pd.DataFrame(data_list, columns=['ear_left', 'ear_right', 'mar', 'label'])
df.to_csv('combined_dataset.csv', index=False)
print(f"âœ… è™•ç†å®Œæˆï¼å…±æå– {len(df)} ç­†æœ‰æ•ˆè³‡æ–™ï¼Œå·²å­˜ç‚º combined_dataset.csv")
æ­¥é©ŸäºŒï¼šè¨“ç·´æ¨¡å‹ (2_train_model.py)
å› ç‚ºè³‡æ–™é‡å¤§ (10è¬ç­†)ï¼Œæˆ‘å€‘ä½¿ç”¨ XGBoostã€‚

Python

# 2_train_model.py
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

# 1. è®€å–æ•´åˆå¾Œçš„è³‡æ–™
print("ğŸ“‚ è®€å–è³‡æ–™é›†...")
try:
    df = pd.read_csv('combined_dataset.csv')
except FileNotFoundError:
    print("âŒ æ‰¾ä¸åˆ° combined_dataset.csvï¼Œè«‹å…ˆåŸ·è¡Œæ­¥é©Ÿä¸€ï¼")
    exit()

print(f"ç¸½è³‡æ–™é‡: {len(df)}")
print("é¡åˆ¥åˆ†ä½ˆ:\n", df['label'].value_counts())

# 2. åˆ‡åˆ†è³‡æ–™
X = df[['ear_left', 'ear_right', 'mar']]
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. è¨“ç·´ (ä½¿ç”¨è¼•é‡åŒ–åƒæ•¸ä»¥é¿å…éæ“¬åˆ)
print("ğŸ§  é–‹å§‹è¨“ç·´ XGBoost...")
model = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    use_label_encoder=False,
    eval_metric='logloss'
)
model.fit(X_train, y_train)

# 4. è©•ä¼°
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\nğŸ† æ¨¡å‹æº–ç¢ºç‡: {acc:.4f}")
print("\nè©³ç´°å ±å‘Š:\n", classification_report(y_test, y_pred))
print("\næ··æ·†çŸ©é™£:\n", confusion_matrix(y_test, y_pred))

# 5. å„²å­˜
joblib.dump(model, 'driver_drowsiness_model.pkl')
print("ğŸ’¾ æ¨¡å‹å·²å„²å­˜ç‚º driver_drowsiness_model.pkl")
æ­¥é©Ÿä¸‰ï¼šStreamlit çµ‚æ¥µ Demo (app.py)
é€™å€‹ App åŒ…å«ï¼š

å´é‚Šæ¬„åˆ‡æ›ï¼šWebcam ç›£æ§ / å½±ç‰‡æª”åˆ†æã€‚

å³æ™‚è­¦ç¤ºæ©Ÿåˆ¶ï¼šä¸åªæ˜¯å–®å¹€åˆ¤æ–·ï¼Œæˆ‘åŠ å…¥äº†ä¸€å€‹ counter æ©Ÿåˆ¶ï¼Œé€£çºŒåµæ¸¬åˆ°ç–²å‹æ‰å ±è­¦ (æ¨¡æ“¬ PERCLOS æ¦‚å¿µ)ï¼Œé¿å…èª¤å ±ã€‚

å½±ç‰‡æ”¯æ´ï¼šå¯ä»¥ä¸Šå‚³ .mp4 é€²è¡Œåˆ†æã€‚

Python

# app.py
import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import joblib
import tempfile
import time
from scipy.spatial import distance as dist
from PIL import Image

# --- é…ç½®èˆ‡è¼‰å…¥ ---
st.set_page_config(page_title="ç–²å‹é§•é§›åµæ¸¬ç³»çµ± Pro", layout="wide", page_icon="ğŸš—")

@st.cache_resource
def load_model():
    return joblib.load('driver_drowsiness_model.pkl')

try:
    model = load_model()
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°æ¨¡å‹æª”ï¼Œè«‹å…ˆåŸ·è¡Œè¨“ç·´ç¨‹å¼ï¼")
    st.stop()

# MediaPipe è¨­å®š
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# é—œéµé»å®šç¾©
LEFT_EYE = [362, 385, 387, 263, 373, 380]
RIGHT_EYE = [33, 160, 158, 133, 153, 144]
MOUTH = [78, 81, 13, 311, 308, 402, 14, 178]

# --- è¼”åŠ©å‡½å¼ ---
def calculate_features(landmarks):
    # EAR
    def eye_aspect_ratio(eye_pts):
        p = [landmarks[i] for i in eye_pts]
        A = dist.euclidean((p[1].x, p[1].y), (p[5].x, p[5].y))
        B = dist.euclidean((p[2].x, p[2].y), (p[4].x, p[4].y))
        C = dist.euclidean((p[0].x, p[0].y), (p[3].x, p[3].y))
        return (A + B) / (2.0 * C) if C != 0 else 0

    # MAR
    def mouth_aspect_ratio(mouth_pts):
        p = [landmarks[i] for i in mouth_pts]
        A = dist.euclidean((p[1].x, p[1].y), (p[7].x, p[7].y))
        B = dist.euclidean((p[2].x, p[2].y), (p[6].x, p[6].y))
        C = dist.euclidean((p[3].x, p[3].y), (p[5].x, p[5].y))
        D = dist.euclidean((p[0].x, p[0].y), (p[4].x, p[4].y))
        return (A + B + C) / (2.0 * D) if D != 0 else 0

    ear_left = eye_aspect_ratio(LEFT_EYE)
    ear_right = eye_aspect_ratio(RIGHT_EYE)
    mar = mouth_aspect_ratio(MOUTH)
    return ear_left, ear_right, mar

def draw_landmarks(image, landmarks):
    h, w, _ = image.shape
    for idx in LEFT_EYE + RIGHT_EYE + MOUTH:
        pt = landmarks[idx]
        cv2.circle(image, (int(pt.x * w), int(pt.y * h)), 1, (0, 255, 255), -1)

# --- ä»‹é¢è¨­è¨ˆ ---
st.title("ğŸš— AI é§•é§›ç–²å‹ç›£æ§ç³»çµ± (NTHU+DDD)")
st.sidebar.title("æ§åˆ¶é¢æ¿")
mode = st.sidebar.radio("é¸æ“‡æ¨¡å¼", ["ğŸ“· å³æ™‚ Webcam ç›£æ§", "ğŸ“‚ å½±ç‰‡æª”æ¡ˆåˆ†æ"])

# ç‹€æ…‹è®Šæ•¸ (ç”¨æ–¼å¹³æ»‘åŒ–é æ¸¬ï¼Œé¿å…é–ƒçˆ)
if 'drowsy_counter' not in st.session_state:
    st.session_state.drowsy_counter = 0

ALARM_TRIGGER_FRAMES = 5  # é€£çºŒ N å¹€åµæ¸¬åˆ°ç–²å‹æ‰å ±è­¦

# --- ä¸»é‚è¼¯: è™•ç†å–®å¹€å½±åƒ ---
def process_frame(frame):
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(rgb_frame)
    
    status = "æª¢æ¸¬ä¸­..."
    color = (255, 255, 0)
    features_info = {"EAR": 0.0, "MAR": 0.0}
    is_drowsy = False

    if results.multi_face_landmarks:
        landmarks = results.multi_face_landmarks[0].landmark
        
        # 1. æå–ç‰¹å¾µ
        ear_l, ear_r, mar = calculate_features(landmarks)
        features_info = {"EAR": (ear_l + ear_r)/2, "MAR": mar}
        
        # 2. è¦–è¦ºåŒ–é—œéµé»
        draw_landmarks(frame, landmarks)
        
        # 3. æ¨¡å‹é æ¸¬
        input_data = np.array([[ear_l, ear_r, mar]])
        prediction = model.predict(input_data)[0]
        
        # 4. è­¦å ±é‚è¼¯ (å¹³æ»‘åŒ–)
        if prediction == 1:
            st.session_state.drowsy_counter += 1
        else:
            st.session_state.drowsy_counter = max(0, st.session_state.drowsy_counter - 1)

        if st.session_state.drowsy_counter >= ALARM_TRIGGER_FRAMES:
            status = "âš ï¸ ç–²å‹é§•é§›è­¦å‘Š! (DROWSY)"
            color = (0, 0, 255) # ç´…è‰²
            is_drowsy = True
        else:
            status = "âœ… ç²¾ç¥ç‹€æ…‹è‰¯å¥½ (ALERT)"
            color = (0, 255, 0) # ç¶ è‰²

    # ç¹ªè£½æ–‡å­—
    cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    cv2.putText(frame, f"EAR: {features_info['EAR']:.2f} | MAR: {features_info['MAR']:.2f}", 
                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    
    return frame, is_drowsy, features_info

# --- æ¨¡å¼ A: Webcam ---
if mode == "ğŸ“· å³æ™‚ Webcam ç›£æ§":
    col1, col2 = st.columns([3, 1])
    with col1:
        st_frame = st.image([])
    with col2:
        st.markdown("### ç‹€æ…‹æ•¸æ“š")
        kpi_status = st.empty()
        kpi_ear = st.metric("å¹³å‡ EAR (çœ¼)", "0.00")
        kpi_mar = st.metric("MAR (å˜´)", "0.00")
    
    run = st.checkbox("å•Ÿå‹•é¡é ­", value=False)
    cap = cv2.VideoCapture(0)
    
    while run:
        ret, frame = cap.read()
        if not ret:
            st.error("ç„¡æ³•è®€å–é¡é ­")
            break
        
        frame = cv2.flip(frame, 1)
        processed_frame, is_drowsy, info = process_frame(frame)
        
        st_frame.image(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))
        
        if is_drowsy:
            kpi_status.error("âš ï¸ è­¦å‘Šï¼")
        else:
            kpi_status.success("æ­£å¸¸")

    cap.release()

# --- æ¨¡å¼ B: å½±ç‰‡åˆ†æ ---
elif mode == "ğŸ“‚ å½±ç‰‡æª”æ¡ˆåˆ†æ":
    uploaded_file = st.sidebar.file_uploader("ä¸Šå‚³å½±ç‰‡æª” (.mp4)", type=["mp4", "avi"])
    
    if uploaded_file is not None:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(uploaded_file.read())
        
        cap = cv2.VideoCapture(tfile.name)
        st_frame = st.image([])
        progress_bar = st.progress(0)
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        curr_frame = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            processed_frame, is_drowsy, info = process_frame(frame)
            st_frame.image(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))
            
            curr_frame += 1
            progress_bar.progress(min(curr_frame / total_frames, 1.0))
            
        cap.release()